\appendix
\chapter{}
\section{Modules}
\subsection{First Module} 
The ﬁrst module is text line normalization. This is important because the MLP classiﬁer takes a ﬁxed dimensional input and text lines differ signiﬁcantly with respect to skew, height and width of the characters. Printed documents originally have zero skew, but when a page is scanned or photocopied, nonzero skew may be introduced. Skew can be corrected at page level but as we are working with text lines, we need to correct any possible skew for every text line before further processing. After skew angle correction text lines are normalized to a height of 30 pixels. In order to normalize the text line, we ﬁrst divide the text line into ascender, descender and middle or x-height regions. This division is performed while estimating the base line, and x-line using linear regression. The heights of ascender and descender regions are made equal to x-height by cropping or padding. These three regions are then rescaled separately to a height of 10 pixels (calculated as desired height). This kind of normalization is performed because we want to rescale the x-height of all characters to a speciﬁc height without affecting the ratio of the x-height to the body height (one of the major characteristics that deﬁnes the appearance of a typeface).\\

\subsection{Second Module}
Pixel based features are extracted from normalized text lines at possible character and non-character positions to provide positive and negative examples from training data. The possible character positions are obtained using a dynamic programming algorithm as proposed by Breuel . A mapping function is used to provide correspondence between characters in normalized text line to the possible character position in original text line. A 30 X 20 (height X width)window is placed at each possible character so that the baseline is at y = 20 and x-height is at y = 10 and the character is at the center of the window. The width of the window is set to 20 pixels to incorporate the neighboring context. This contextual window is moved from one possible character to another possible character to extract feature vectors for valid characters. Feature vectors for non-character/garbage class are obtained by placing the window at center of two consecutive characters. Spaces are considered as valid characters and distinction between space and garbage class is made by computing the distance between two consecutive characters. If the distance is less than a speciﬁc threshold value then it is considered as garbage, otherwise it is considered as a space. Due to variations in inter-character spaces this threshold is computed for every text line. A mean distance between all the characters in a text line is computed and a standard deviation is added to that mean. This sum of mean and standard deviation provides the threshold value for spaces. At each 30 X 20 contextual window, gray scale pixel values are used to construct the feature vector.\\

\subsection{Third Module}
Artiﬁcial neural networks (ANNs) have been successfully applied for character recognition. One of the long-standing problems related to ANNs is parameter optimization, such as selection of learning rate, numbers of hidden units and epochs. To avoid these problems, we use an auto tunable multilayer perceptron (AutoMLP)  for training and recognition. The AutoMLP works by combining the ideas from genetic algorithms and stochastic optimization. It maintains a small ensemble of networks that are trained in parallel with different learning rates and different numbers of hidden units using gradient based optimization algorithms. After a small, ﬁxed number of epochs, the error rate is determined on a validation set. The worst performer neural networks are replaced with copies of the best networks, modiﬁed to have different numbers of hidden units and learning rates. The extracted features are used to train AutoMLP for 94 character classes–upper and lower case Latin characters, numerals, punctuation marks and white space– along with one extra garbage class. Hence the network has 95 output units. The activations of the output layer can be now interpreted as the probabilities of observing the valid character classes as well as the probability of observing garbage at a particular position on a text line. This leads us to the idea of line scanning neural network.\\

\subsection{Fourth Module}
The line scanning neural network works by moving a contextual window, from left to right, centered at each pixel position on a normalized text line. The output of the line scanning neural network is a vector of posterior probabilities (one element for each character class). A character sequence can also be generated by picking the most probable class from these output probabilities by detecting the local maximum (peak).\\

\subsection{Fifth Module}
Hidden Markov Models (HMMs) have been successfully applied to handwritten and cursive script text recognition. The basic idea is that the output of line scanning neural network can be interpreted as a left-to-right sequence of signals. Therefore, the output vector generated by scanning neural network is treated as the observations for Gaussian mixture based HMMs. As the output probabilities have very skewed distribution, the probabilities are smoothed with a Gaussian kernel (σ = 0.5) and are converted to negative logs before passing them as feature inputs to HMMs. The presented method models the character classes with multi-state, left to right, continuous density HMMs. Each character model has 10 states with 256 Gaussian mixture densities, self loops and transition to adjacent states with one skip. The number of states and mixture densities are determined empirically on a small set of validation data. The “start” and “end” are non-emitting states and are used to provide transitions from one character model to the other character model. The text lines are modeled by concatenating these character models in ergodic structure. Training or estimating the HMM parameters is performed using Baum-Welch re-estimation algorithm, which iteratively aligns the feature vectors with the character models in a maximum likelihood sense.\\
